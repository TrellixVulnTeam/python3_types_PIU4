# (generated with --quick)

from typing import Any, Callable, Optional, Set

FromParams: Any
Model: Any
SummaryWriter: Any
logger: logging.Logger
logging: module
os: module
torch: Any

class TensorboardWriter(Any):
    __doc__: str
    _get_batch_num_total: Callable[[], int]
    _histogram_interval: Optional[int]
    _should_log_learning_rate: bool
    _should_log_parameter_statistics: bool
    _summary_interval: int
    _train_log: Any
    _validation_log: Any
    def __init__(self, get_batch_num_total: Callable[[], int], serialization_dir: Optional[str] = ..., summary_interval: int = ..., histogram_interval: Optional[int] = ..., should_log_parameter_statistics: bool = ..., should_log_learning_rate: bool = ...) -> None: ...
    @staticmethod
    def _item(value) -> Any: ...
    def add_train_histogram(self, name: str, values) -> None: ...
    def add_train_scalar(self, name: str, value: float, timestep: Optional[int] = ...) -> None: ...
    def add_validation_scalar(self, name: str, value: float, timestep: Optional[int] = ...) -> None: ...
    def close(self) -> None: ...
    def enable_activation_logging(self, model) -> None: ...
    def log_activation_histogram(self, outputs, log_prefix: str) -> None: ...
    def log_histograms(self, model, histogram_parameters: Set[str]) -> None: ...
    def log_learning_rates(self, model, optimizer) -> None: ...
    def log_metrics(self, train_metrics: dict, val_metrics: Optional[dict] = ..., epoch: Optional[int] = ..., log_to_console: bool = ...) -> None: ...
    def log_parameter_and_gradient_statistics(self, model, batch_grad_norm: float) -> None: ...
    def should_log_histograms_this_batch(self) -> bool: ...
    def should_log_this_batch(self) -> bool: ...
