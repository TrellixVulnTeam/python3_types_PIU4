from rlkit.torch.torch_rl_algorithm import TorchTrainer
from typing import Any, Optional

class DDPGTrainer(TorchTrainer):
    qf: Any = ...
    target_qf: Any = ...
    policy: Any = ...
    target_policy: Any = ...
    discount: Any = ...
    reward_scale: Any = ...
    policy_learning_rate: Any = ...
    qf_learning_rate: Any = ...
    qf_weight_decay: Any = ...
    target_hard_update_period: Any = ...
    tau: Any = ...
    use_soft_update: Any = ...
    qf_criterion: Any = ...
    policy_pre_activation_weight: Any = ...
    min_q_value: Any = ...
    max_q_value: Any = ...
    qf_optimizer: Any = ...
    policy_optimizer: Any = ...
    eval_statistics: Any = ...
    _n_train_steps_total: int = ...
    _need_to_update_eval_statistics: bool = ...
    def __init__(self, qf: Any, target_qf: Any, policy: Any, target_policy: Any, discount: float = ..., reward_scale: float = ..., policy_learning_rate: float = ..., qf_learning_rate: float = ..., qf_weight_decay: int = ..., target_hard_update_period: int = ..., tau: float = ..., use_soft_update: bool = ..., qf_criterion: Optional[Any] = ..., policy_pre_activation_weight: float = ..., optimizer_class: Any = ..., min_q_value: Any = ..., max_q_value: Any = ...) -> None: ...
    def train_from_torch(self, batch: Any) -> None: ...
    def _update_target_networks(self) -> None: ...
    def get_diagnostics(self): ...
    def end_epoch(self, epoch: Any) -> None: ...
    @property
    def networks(self): ...
    def get_epoch_snapshot(self): ...
