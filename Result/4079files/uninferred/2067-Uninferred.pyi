from typing import Any

def mask_seq(inputs: Any, sequence_length: Any, name: str = ...): ...
def bias_seq(inputs: Any, sequence_length: Any, bias: Any = ..., name: str = ...): ...

class ContentAttention:
    memory: Any = ...
    sequence_length: Any = ...
    units: Any = ...
    batch_size: Any = ...
    enc_length: Any = ...
    enc_units: Any = ...
    alignments_history: Any = ...
    name: Any = ...
    hidden_feats: Any = ...
    e_range_seq: Any = ...
    e_bias_zero: Any = ...
    e_bias_neg: Any = ...
    def __init__(self, memory: Any, sequence_length: Any, units: Any, alignments_history: bool = ..., name: str = ...) -> None: ...
    def zero_state(self, batch_size: Any, dtype: Any): ...
    def __call__(self, query: Any, state_tm1: Any): ...

class GMMAttention:
    memory: Any = ...
    sequence_length: Any = ...
    units: Any = ...
    batch_size: Any = ...
    enc_length: Any = ...
    enc_units: Any = ...
    alignments_history: Any = ...
    init_kappa_pos: Any = ...
    name: Any = ...
    tmp_l: Any = ...
    mask: Any = ...
    def __init__(self, memory: Any, sequence_length: Any, units: Any, alignments_history: bool = ..., init_kappa_pos: Any = ..., name: str = ...) -> None: ...
    def zero_state(self, batch_size: Any, dtype: Any): ...
    def __call__(self, query: Any, state_tm1: Any): ...

class MultiHeadAttentionWrapper:
    att_mod_list: Any = ...
    name: Any = ...
    def __init__(self, att_mod_list: Any, name: str = ...) -> None: ...
    def zero_state(self, batch_size: Any, dtype: Any): ...
    def __call__(self, query: Any, state_lst_tm1: Any, reuse: Any = ...): ...
